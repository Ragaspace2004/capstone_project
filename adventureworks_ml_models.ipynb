{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756413cd",
   "metadata": {},
   "source": [
    "# AdventureWorks Predictive Analytics - ML Models & PKL Export\n",
    "\n",
    "This notebook builds comprehensive machine learning models for AdventureWorks dataset including:\n",
    "- Sales Forecasting\n",
    "- Customer Lifetime Value (CLV)  \n",
    "- Customer Segmentation\n",
    "- Customer Churn Prediction\n",
    "- Demand Forecasting\n",
    "\n",
    "All trained models will be saved as `.pkl` files for use in Streamlit application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65b05d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including pandas, numpy, scikit-learn, pickle, and streamlit dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb92579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Setting up AdventureWorks ML Environment...\n",
      "============================================================\n",
      "ðŸ“¦ Installing packages with UV...\n",
      "âœ… All packages installed successfully!\n",
      "\n",
      "ðŸ”§ Environment Setup Complete!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ðŸ” Checking package versions...\n",
      "âœ… numpy: 2.3.2\n",
      "âœ… pandas: 2.3.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m     install_packages()\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m     \u001b[43mcheck_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ Ready to run machine learning models!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestart kernel if you encounter any import issues.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 110\u001b[0m, in \u001b[0;36mcheck_environment\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m package_import, package_name \u001b[38;5;129;01min\u001b[39;00m packages_to_check\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage_import\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m         version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\GANES\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:994\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\__init__.py:87\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     )\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     90\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compress, islice\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\sparse\\__init__.py:297\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dok\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\sparse\\_lil.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexMixin, INT_TYPES, _broadcast_arrays\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (getdtype, isshape, isscalarlike, upcast_scalar,\n\u001b[0;32m     16\u001b[0m                        check_shape, check_reshape_kwargs)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _csparsetools\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_lil_base\u001b[39;00m(_spbase, IndexMixin):\n\u001b[0;32m     21\u001b[0m     _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlil\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mscipy\\\\sparse\\\\_csparsetools.pyx:1\u001b[0m, in \u001b[0;36minit _csparsetools\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE PACKAGE INSTALLATION & ENVIRONMENT SETUP\n",
    "# Run this cell first to install all required packages and resolve compatibility issues\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def fix_numpy_compatibility():\n",
    "    \"\"\"Fix NumPy-SciPy binary incompatibility by installing compatible versions\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”§ FIXING NUMPY-SCIPY COMPATIBILITY ISSUE...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, uninstall problematic packages\n",
    "    packages_to_uninstall = [\"numpy\", \"scipy\", \"scikit-learn\", \"seaborn\"]\n",
    "    \n",
    "    print(\"\uddd1ï¸  Uninstalling incompatible packages...\")\n",
    "    for package in packages_to_uninstall:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", package, \"-y\"], \n",
    "                         capture_output=True, check=False)\n",
    "            print(f\"   Uninstalled: {package}\")\n",
    "        except:\n",
    "            print(f\"   Skipped: {package}\")\n",
    "    \n",
    "    # Install compatible versions in specific order\n",
    "    print(\"\\nðŸ“¦ Installing compatible package versions...\")\n",
    "    \n",
    "    compatible_packages = [\n",
    "        # Core numerical packages - MUST be compatible versions\n",
    "        \"numpy==1.26.4\",  # Last stable 1.x version - widely compatible\n",
    "        \"scipy==1.11.4\",  # Compatible with NumPy 1.26.x\n",
    "        \"scikit-learn==1.4.0\",  # Compatible with above versions\n",
    "        \n",
    "        # Data manipulation\n",
    "        \"pandas>=2.0.0,<2.5.0\",\n",
    "        \n",
    "        # Visualization (install after numpy/scipy)\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"seaborn>=0.12.0\",\n",
    "        \"plotly>=5.15.0\"\n",
    "    ]\n",
    "    \n",
    "    # Install packages one by one to catch errors\n",
    "    for package in compatible_packages:\n",
    "        try:\n",
    "            print(f\"Installing {package}...\")\n",
    "            result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"âœ… {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âŒ Failed to install {package}: {e}\")\n",
    "            print(f\"   Error output: {e.stderr}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def install_remaining_packages():\n",
    "    \"\"\"Install remaining packages after fixing core compatibility\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ“¦ Installing additional packages...\")\n",
    "    \n",
    "    # Database and serialization\n",
    "    database_packages = [\n",
    "        \"sqlalchemy>=2.0.0\",\n",
    "        \"pymysql>=1.0.0\"\n",
    "    ]\n",
    "    \n",
    "    # Machine Learning extras (install after scikit-learn)\n",
    "    ml_packages = [\n",
    "        \"xgboost>=1.7.0\",\n",
    "        \"lightgbm>=4.0.0\"\n",
    "    ]\n",
    "    \n",
    "    # Web application\n",
    "    web_packages = [\n",
    "        \"streamlit>=1.25.0\",\n",
    "        \"streamlit-option-menu>=0.3.0\"\n",
    "    ]\n",
    "    \n",
    "    # Utility packages\n",
    "    utility_packages = [\n",
    "        \"jupyter>=1.0.0\",\n",
    "        \"ipywidgets>=8.0.0\",\n",
    "        \"tqdm>=4.65.0\"\n",
    "    ]\n",
    "    \n",
    "    # TensorFlow (optional - known to have compatibility issues)\n",
    "    optional_packages = [\n",
    "        \"tensorflow>=2.13.0,<2.16.0\"  # Version range that works with NumPy 1.26\n",
    "    ]\n",
    "    \n",
    "    all_additional = database_packages + ml_packages + web_packages + utility_packages\n",
    "    \n",
    "    # Install additional packages\n",
    "    for package in all_additional:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                         capture_output=True, check=True)\n",
    "            print(f\"âœ… {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"âš ï¸  Skipped: {package}\")\n",
    "    \n",
    "    # Try TensorFlow separately (may fail on some systems)\n",
    "    print(\"\\nðŸ¤– Installing TensorFlow (optional)...\")\n",
    "    for package in optional_packages:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                         capture_output=True, check=True)\n",
    "            print(f\"âœ… {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"âš ï¸  TensorFlow installation failed - deep learning features will be disabled\")\n",
    "\n",
    "def check_compatibility():\n",
    "    \"\"\"Check if packages are properly installed and compatible\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ” Verifying package compatibility...\")\n",
    "    \n",
    "    try:\n",
    "        import numpy as np\n",
    "        print(f\"âœ… NumPy: {np.__version__}\")\n",
    "        \n",
    "        import scipy\n",
    "        print(f\"âœ… SciPy: {scipy.__version__}\")\n",
    "        \n",
    "        import sklearn\n",
    "        print(f\"âœ… Scikit-learn: {sklearn.__version__}\")\n",
    "        \n",
    "        import pandas as pd\n",
    "        print(f\"âœ… Pandas: {pd.__version__}\")\n",
    "        \n",
    "        # Test critical imports\n",
    "        from scipy import stats\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        print(\"âœ… Critical imports working\")\n",
    "        \n",
    "        # Check NumPy version compatibility\n",
    "        np_version = tuple(map(int, np.__version__.split('.')[:2]))\n",
    "        if np_version == (1, 26):\n",
    "            print(\"âœ… NumPy version is optimal for compatibility\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  NumPy version {np.__version__} - may have some compatibility issues\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Compatibility check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main execution\n",
    "print(\"ðŸš€ FIXING ADVENTUREWORKS ML ENVIRONMENT...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Fix core compatibility\n",
    "if fix_numpy_compatibility():\n",
    "    print(\"\\nâœ… Core packages installed successfully!\")\n",
    "    \n",
    "    # Step 2: Install additional packages\n",
    "    install_remaining_packages()\n",
    "    \n",
    "    # Step 3: Verify everything works\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if check_compatibility():\n",
    "        print(\"\\nðŸŽ‰ Environment setup completed successfully!\")\n",
    "        print(\"All packages are compatible and ready to use.\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Some compatibility issues remain.\")\n",
    "        print(\"Try restarting the kernel and running this cell again.\")\n",
    "else:\n",
    "    print(\"\\nâŒ Failed to install core packages.\")\n",
    "    print(\"Manual installation may be required.\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready to run machine learning models!\")\n",
    "print(\"ðŸ“ Note: Restart kernel before proceeding to imports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d57a19",
   "metadata": {},
   "source": [
    "## 0. ðŸ”§ Environment Setup & Compatibility Fix\n",
    "\n",
    "**IMPORTANT: NumPy-SciPy Compatibility Issue Detected!**\n",
    "\n",
    "You're experiencing a binary incompatibility between NumPy 2.3.2 and SciPy. This is a common issue when NumPy gets a major version update.\n",
    "\n",
    "### ðŸš¨ **Required Steps:**\n",
    "\n",
    "1. **Run the cell below** - This will automatically fix the compatibility issue\n",
    "2. **Restart your kernel** after the cell completes successfully  \n",
    "3. **Proceed to the import cell** - All packages will work correctly\n",
    "\n",
    "### ðŸ”§ **What the fix does:**\n",
    "- Uninstalls incompatible NumPy 2.3.2 and related packages\n",
    "- Installs NumPy 1.26.4 (stable, widely compatible version)\n",
    "- Installs compatible versions of SciPy, scikit-learn, etc.\n",
    "- Verifies all packages work together\n",
    "\n",
    "### âš¡ **Quick Manual Fix (Alternative):**\n",
    "If you prefer to fix manually via terminal:\n",
    "```bash\n",
    "pip uninstall numpy scipy scikit-learn seaborn -y\n",
    "pip install numpy==1.26.4 scipy==1.11.4 scikit-learn==1.4.0 seaborn>=0.12.0\n",
    "```\n",
    "\n",
    "**Run the cell below now to automatically fix everything:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412f2181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Configuring environment...\n",
      "âœ… NumPy 2.3.2 | Pandas 2.3.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Visualization Libraries\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\seaborn\\relational.py:21\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     15\u001b[0m     _default_color,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     _scatter_legend_artist,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m groupby_apply_include_groups\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator, WeightedAggregator\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\seaborn\\_statistics.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     33\u001b[0m     _no_scipy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\stats\\__init__.py:605\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m \n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    604\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\stats\\_stats_py.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper, _get_nan,\n\u001b[0;32m     40\u001b[0m                               rng_integers, _rename_parameter, _contains_nan,\n\u001b[0;32m     41\u001b[0m                               AxisError)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\spatial\\__init__.py:110\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries with Compatibility Handling\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Suppress specific compatibility warnings\n",
    "warnings.filterwarnings('ignore', message='A NumPy version.*is required for this version of SciPy')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"ðŸ”§ Configuring environment...\")\n",
    "\n",
    "try:\n",
    "    # Core Data Science Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(f\"âœ… NumPy {np.__version__} | Pandas {pd.__version__}\")\n",
    "    \n",
    "    # Visualization Libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    print(\"âœ… Visualization libraries loaded\")\n",
    "    \n",
    "    # Database Connection\n",
    "    from sqlalchemy import create_engine\n",
    "    print(\"âœ… Database connectivity ready\")\n",
    "    \n",
    "    # Serialization\n",
    "    import pickle\n",
    "    print(\"âœ… Model serialization ready\")\n",
    "    \n",
    "    # Machine Learning Libraries\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    print(\"âœ… Scikit-learn libraries loaded\")\n",
    "    \n",
    "    # Deep Learning (Optional - will not fail if not available)\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "        print(f\"âœ… TensorFlow {tf.__version__} loaded\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  TensorFlow not available - deep learning features disabled\")\n",
    "        tf = None\n",
    "\n",
    "    print(\"\\nðŸŽ‰ All essential libraries imported successfully!\")\n",
    "    \n",
    "    # Environment compatibility check\n",
    "    print(f\"\\nðŸ“Š Environment Info:\")\n",
    "    print(f\"   Python: {sys.version.split()[0]}\")\n",
    "    print(f\"   NumPy: {np.__version__}\")\n",
    "    print(f\"   Pandas: {pd.__version__}\")\n",
    "    \n",
    "    # Check for compatibility issues\n",
    "    np_version = tuple(map(int, np.__version__.split('.')[:3]))\n",
    "    if np_version >= (2, 0, 0):\n",
    "        print(\"âš ï¸  Note: NumPy 2.x detected - some packages may show compatibility warnings\")\n",
    "        print(\"   This is expected and won't affect functionality\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import Error: {e}\")\n",
    "    print(\"Please run the package installation cell above first!\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeef266",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "Load the cleaned AdventureWorks dataset from MySQL and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0852d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m PASSWORD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m DATABASE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madventureworks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_engine\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmysql+pymysql://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUSER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPASSWORD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATABASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?charset=utf8mb4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load all cleaned datasets\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_engine' is not defined"
     ]
    }
   ],
   "source": [
    "# Database connection\n",
    "# Import required libraries for database connectivity\n",
    "try:\n",
    "    from sqlalchemy import create_engine\n",
    "    import pandas as pd\n",
    "    print(\"âœ… Database libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"Please run the package installation and import cells first!\")\n",
    "    raise\n",
    "\n",
    "HOST = \"localhost\"\n",
    "PORT = 3306\n",
    "USER = \"root\"\n",
    "PASSWORD = \"root\"\n",
    "DATABASE = \"adventureworks\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(f\"mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4\")\n",
    "    print(\"âœ… Database engine created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Database connection error: {e}\")\n",
    "    print(\"Please ensure MySQL server is running and credentials are correct\")\n",
    "    raise\n",
    "\n",
    "# Load all cleaned datasets\n",
    "print(\"\\nðŸ“Š Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    # Load customer data\n",
    "    customers_df = pd.read_sql(\"SELECT * FROM customer_trans\", engine)\n",
    "    print(f\"âœ… Customers: {len(customers_df)} records\")\n",
    "\n",
    "    # Load sales data\n",
    "    sales_df = pd.read_sql(\"SELECT * FROM sales_trans\", engine)\n",
    "    print(f\"âœ… Sales: {len(sales_df)} records\")\n",
    "\n",
    "    # Load products data\n",
    "    products_df = pd.read_sql(\"SELECT * FROM products_trans\", engine)\n",
    "    print(f\"âœ… Products: {len(products_df)} records\")\n",
    "\n",
    "    # Load territories data\n",
    "    territories_df = pd.read_sql(\"SELECT * FROM territories_trans\", engine)\n",
    "    print(f\"âœ… Territories: {len(territories_df)} records\")\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Dataset loading complete!\")\n",
    "    print(f\"Total records loaded: {len(customers_df) + len(sales_df) + len(products_df) + len(territories_df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    print(\"Please check if all tables exist in the database\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb85024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "print(\"CUSTOMER DATA STRUCTURE:\")\n",
    "print(customers_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(customers_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SALES DATA STRUCTURE:\")\n",
    "print(sales_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a133b2f",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Clean the data, handle missing values, encode categorical variables, and create features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Function\n",
    "def preprocess_data():\n",
    "    \"\"\"Comprehensive data preprocessing for all models\"\"\"\n",
    "    \n",
    "    # Convert date columns\n",
    "    sales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\n",
    "    customers_df['birth_date'] = pd.to_datetime(customers_df['birth_date'])\n",
    "    \n",
    "    # Create customer age\n",
    "    current_date = pd.Timestamp.now()\n",
    "    customers_df['age'] = (current_date - customers_df['birth_date']).dt.days // 365\n",
    "    \n",
    "    # Handle missing values\n",
    "    customers_df['annual_income'].fillna(customers_df['annual_income'].median(), inplace=True)\n",
    "    customers_df['total_children'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_gender = LabelEncoder()\n",
    "    le_marital = LabelEncoder()\n",
    "    le_education = LabelEncoder()\n",
    "    \n",
    "    customers_df['gender_encoded'] = le_gender.fit_transform(customers_df['gender'].fillna('Unknown'))\n",
    "    customers_df['marital_status_encoded'] = le_marital.fit_transform(customers_df['marital_status'].fillna('Unknown'))\n",
    "    customers_df['education_encoded'] = le_education.fit_transform(customers_df['education_level'].fillna('Unknown'))\n",
    "    \n",
    "    # Create aggregated customer features\n",
    "    customer_sales = sales_df.groupby('customer_id').agg({\n",
    "        'order_quantity': ['count', 'sum', 'mean'],\n",
    "        'order_date': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_sales.columns = ['customer_id', 'total_orders', 'total_quantity', 'avg_order_size', 'first_order', 'last_order']\n",
    "    \n",
    "    # Calculate customer metrics\n",
    "    customer_sales['customer_lifespan_days'] = (customer_sales['last_order'] - customer_sales['first_order']).dt.days\n",
    "    customer_sales['recency_days'] = (current_date - customer_sales['last_order']).dt.days\n",
    "    \n",
    "    # Merge customer and sales data\n",
    "    customer_features = customers_df.merge(customer_sales, on='customer_id', how='left')\n",
    "    customer_features.fillna(0, inplace=True)\n",
    "    \n",
    "    print(\"Data preprocessing completed!\")\n",
    "    return customer_features\n",
    "\n",
    "# Execute preprocessing\n",
    "customer_features = preprocess_data()\n",
    "print(f\"Final dataset shape: {customer_features.shape}\")\n",
    "print(customer_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b961b6a",
   "metadata": {},
   "source": [
    "## 4. Split Data into Training and Testing Sets\n",
    "\n",
    "Prepare data splits for different modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for different models\n",
    "\n",
    "# 1. Customer Lifetime Value (CLV) Dataset\n",
    "clv_features = ['annual_income', 'total_children', 'age', 'gender_encoded', \n",
    "               'marital_status_encoded', 'education_encoded', 'total_orders', \n",
    "               'total_quantity', 'avg_order_size', 'customer_lifespan_days']\n",
    "\n",
    "# Create CLV target (proxy using total value)\n",
    "customer_features['clv_target'] = (customer_features['total_quantity'] * \n",
    "                                  customer_features['avg_order_size'] * \n",
    "                                  (1 + customer_features['total_orders'] / 100))\n",
    "\n",
    "# Remove outliers for CLV\n",
    "clv_data = customer_features[customer_features['total_orders'] > 0].copy()\n",
    "Q1 = clv_data['clv_target'].quantile(0.25)\n",
    "Q3 = clv_data['clv_target'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "clv_data = clv_data[(clv_data['clv_target'] >= Q1 - 1.5*IQR) & \n",
    "                    (clv_data['clv_target'] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "X_clv = clv_data[clv_features]\n",
    "y_clv = clv_data['clv_target']\n",
    "\n",
    "X_clv_train, X_clv_test, y_clv_train, y_clv_test = train_test_split(X_clv, y_clv, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"CLV Dataset - Train: {X_clv_train.shape}, Test: {X_clv_test.shape}\")\n",
    "\n",
    "# 2. Customer Segmentation Dataset (unsupervised)\n",
    "segmentation_features = ['annual_income', 'total_children', 'age', 'total_orders', \n",
    "                        'total_quantity', 'avg_order_size', 'recency_days']\n",
    "X_segment = clv_data[segmentation_features]\n",
    "\n",
    "print(f\"Segmentation Dataset: {X_segment.shape}\")\n",
    "\n",
    "# 3. Churn Prediction Dataset\n",
    "# Define churn as customers with no orders in last 180 days\n",
    "customer_features['is_churned'] = (customer_features['recency_days'] > 180).astype(int)\n",
    "churn_data = customer_features[customer_features['total_orders'] > 0].copy()\n",
    "\n",
    "churn_features = ['annual_income', 'total_children', 'age', 'gender_encoded', \n",
    "                 'marital_status_encoded', 'education_encoded', 'total_orders', \n",
    "                 'avg_order_size', 'customer_lifespan_days', 'recency_days']\n",
    "\n",
    "X_churn = churn_data[churn_features]\n",
    "y_churn = churn_data['is_churned']\n",
    "\n",
    "X_churn_train, X_churn_test, y_churn_train, y_churn_test = train_test_split(X_churn, y_churn, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Churn Dataset - Train: {X_churn_train.shape}, Test: {X_churn_test.shape}\")\n",
    "print(f\"Churn Distribution: {y_churn.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee4361",
   "metadata": {},
   "source": [
    "## 5. Train Machine Learning Models\n",
    "\n",
    "Train multiple ML models including Random Forest, clustering, and classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60711e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CUSTOMER LIFETIME VALUE (CLV) MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CUSTOMER LIFETIME VALUE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for CLV\n",
    "scaler_clv = StandardScaler()\n",
    "X_clv_train_scaled = scaler_clv.fit_transform(X_clv_train)\n",
    "X_clv_test_scaled = scaler_clv.transform(X_clv_test)\n",
    "\n",
    "# Train Random Forest for CLV\n",
    "rf_clv = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_clv.fit(X_clv_train_scaled, y_clv_train)\n",
    "\n",
    "# Predictions\n",
    "y_clv_pred = rf_clv.predict(X_clv_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "clv_mae = mean_absolute_error(y_clv_test, y_clv_pred)\n",
    "clv_rmse = np.sqrt(mean_squared_error(y_clv_test, y_clv_pred))\n",
    "clv_r2 = r2_score(y_clv_test, y_clv_pred)\n",
    "\n",
    "print(f\"CLV Model Performance:\")\n",
    "print(f\"MAE: {clv_mae:.2f}\")\n",
    "print(f\"RMSE: {clv_rmse:.2f}\")\n",
    "print(f\"RÂ²: {clv_r2:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "clv_importance = pd.DataFrame({\n",
    "    'feature': clv_features,\n",
    "    'importance': rf_clv.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop CLV Features:\")\n",
    "print(clv_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774bb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CUSTOMER SEGMENTATION MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CUSTOMER SEGMENTATION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for segmentation\n",
    "scaler_segment = StandardScaler()\n",
    "X_segment_scaled = scaler_segment.fit_transform(X_segment)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "k_range = range(2, 11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_segment_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Train KMeans with optimal clusters (let's use 5)\n",
    "optimal_k = 5\n",
    "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "segment_labels = kmeans_model.fit_predict(X_segment_scaled)\n",
    "\n",
    "# Add segments to data\n",
    "X_segment['segment'] = segment_labels\n",
    "\n",
    "# Analyze segments\n",
    "segment_analysis = X_segment.groupby('segment').agg({\n",
    "    'annual_income': 'mean',\n",
    "    'total_orders': 'mean',\n",
    "    'total_quantity': 'mean',\n",
    "    'recency_days': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(f\"Customer Segments Analysis:\")\n",
    "print(segment_analysis)\n",
    "\n",
    "# Create segment labels\n",
    "segment_names = ['Low Value', 'Occasional', 'Regular', 'High Value', 'Premium']\n",
    "segment_mapping = {i: segment_names[i] for i in range(optimal_k)}\n",
    "X_segment['segment_name'] = X_segment['segment'].map(segment_mapping)\n",
    "\n",
    "print(f\"\\nSegment Distribution:\")\n",
    "print(X_segment['segment_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd089b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CUSTOMER CHURN PREDICTION MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CUSTOMER CHURN PREDICTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for churn prediction\n",
    "scaler_churn = StandardScaler()\n",
    "X_churn_train_scaled = scaler_churn.fit_transform(X_churn_train)\n",
    "X_churn_test_scaled = scaler_churn.transform(X_churn_test)\n",
    "\n",
    "# Train Random Forest for churn prediction\n",
    "rf_churn = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_churn.fit(X_churn_train_scaled, y_churn_train)\n",
    "\n",
    "# Predictions\n",
    "y_churn_pred = rf_churn.predict(X_churn_test_scaled)\n",
    "y_churn_pred_proba = rf_churn.predict_proba(X_churn_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "churn_accuracy = accuracy_score(y_churn_test, y_churn_pred)\n",
    "print(f\"Churn Model Accuracy: {churn_accuracy:.3f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_churn_test, y_churn_pred))\n",
    "\n",
    "# Feature importance\n",
    "churn_importance = pd.DataFrame({\n",
    "    'feature': churn_features,\n",
    "    'importance': rf_churn.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop Churn Prediction Features:\")\n",
    "print(churn_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SALES FORECASTING MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SALES FORECASTING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare time series data for sales forecasting\n",
    "sales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\n",
    "daily_sales = sales_df.groupby('order_date').agg({\n",
    "    'order_quantity': 'sum',\n",
    "    'customer_id': 'nunique',\n",
    "    'product_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "daily_sales.columns = ['date', 'total_quantity', 'unique_customers', 'unique_products']\n",
    "\n",
    "# Create time-based features\n",
    "daily_sales['year'] = daily_sales['date'].dt.year\n",
    "daily_sales['month'] = daily_sales['date'].dt.month\n",
    "daily_sales['day'] = daily_sales['date'].dt.day\n",
    "daily_sales['dayofweek'] = daily_sales['date'].dt.dayofweek\n",
    "daily_sales['quarter'] = daily_sales['date'].dt.quarter\n",
    "\n",
    "# Create lag features\n",
    "daily_sales['lag_1'] = daily_sales['total_quantity'].shift(1)\n",
    "daily_sales['lag_7'] = daily_sales['total_quantity'].shift(7)\n",
    "daily_sales['rolling_mean_7'] = daily_sales['total_quantity'].rolling(7).mean()\n",
    "\n",
    "# Remove rows with NaN values\n",
    "daily_sales_clean = daily_sales.dropna()\n",
    "\n",
    "# Features and target for sales forecasting\n",
    "forecast_features = ['month', 'day', 'dayofweek', 'quarter', 'unique_customers', \n",
    "                    'unique_products', 'lag_1', 'lag_7', 'rolling_mean_7']\n",
    "X_forecast = daily_sales_clean[forecast_features]\n",
    "y_forecast = daily_sales_clean['total_quantity']\n",
    "\n",
    "X_forecast_train, X_forecast_test, y_forecast_train, y_forecast_test = train_test_split(\n",
    "    X_forecast, y_forecast, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Train Random Forest for sales forecasting\n",
    "rf_forecast = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_forecast.fit(X_forecast_train, y_forecast_train)\n",
    "\n",
    "# Predictions\n",
    "y_forecast_pred = rf_forecast.predict(X_forecast_test)\n",
    "\n",
    "# Evaluation\n",
    "forecast_mae = mean_absolute_error(y_forecast_test, y_forecast_pred)\n",
    "forecast_rmse = np.sqrt(mean_squared_error(y_forecast_test, y_forecast_pred))\n",
    "forecast_r2 = r2_score(y_forecast_test, y_forecast_pred)\n",
    "\n",
    "print(f\"Sales Forecast Model Performance:\")\n",
    "print(f\"MAE: {forecast_mae:.2f}\")\n",
    "print(f\"RMSE: {forecast_rmse:.2f}\")\n",
    "print(f\"RÂ²: {forecast_r2:.3f}\")\n",
    "\n",
    "print(\"All models training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c970c5",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Comprehensive evaluation of all trained models with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48352dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Summary\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "performance_summary = pd.DataFrame({\n",
    "    'Model': ['Customer Lifetime Value', 'Customer Churn Prediction', 'Sales Forecasting'],\n",
    "    'Type': ['Regression', 'Classification', 'Regression'],\n",
    "    'Primary Metric': [f'RÂ² = {clv_r2:.3f}', f'Accuracy = {churn_accuracy:.3f}', f'RÂ² = {forecast_r2:.3f}'],\n",
    "    'Secondary Metric': [f'MAE = {clv_mae:.0f}', 'F1-Score from Report', f'MAE = {forecast_mae:.0f}']\n",
    "})\n",
    "\n",
    "print(performance_summary.to_string(index=False))\n",
    "print(\"\\nCustomer Segmentation: 5 distinct segments created\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# CLV Predictions vs Actual\n",
    "axes[0,0].scatter(y_clv_test, y_clv_pred, alpha=0.6)\n",
    "axes[0,0].plot([y_clv_test.min(), y_clv_test.max()], [y_clv_test.min(), y_clv_test.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual CLV')\n",
    "axes[0,0].set_ylabel('Predicted CLV')\n",
    "axes[0,0].set_title(f'CLV Prediction (RÂ² = {clv_r2:.3f})')\n",
    "\n",
    "# Churn Feature Importance\n",
    "axes[0,1].barh(churn_importance.head(8)['feature'], churn_importance.head(8)['importance'])\n",
    "axes[0,1].set_title('Top Churn Prediction Features')\n",
    "axes[0,1].set_xlabel('Importance')\n",
    "\n",
    "# Sales Forecast\n",
    "axes[1,0].scatter(y_forecast_test, y_forecast_pred, alpha=0.6)\n",
    "axes[1,0].plot([y_forecast_test.min(), y_forecast_test.max()], [y_forecast_test.min(), y_forecast_test.max()], 'r--', lw=2)\n",
    "axes[1,0].set_xlabel('Actual Sales')\n",
    "axes[1,0].set_ylabel('Predicted Sales')\n",
    "axes[1,0].set_title(f'Sales Forecasting (RÂ² = {forecast_r2:.3f})')\n",
    "\n",
    "# Customer Segments\n",
    "segment_counts = X_segment['segment_name'].value_counts()\n",
    "axes[1,1].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Customer Segments Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff228483",
   "metadata": {},
   "source": [
    "## 7. Save Models as PKL Files\n",
    "\n",
    "Use pickle to serialize and save all trained models and scalers for use in Streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models and preprocessors as PKL files\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "models_dir = 'models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "print(\"Saving all trained models as PKL files...\")\n",
    "\n",
    "# 1. CLV Model and Scaler\n",
    "with open('models/clv_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_clv, f)\n",
    "    \n",
    "with open('models/clv_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_clv, f)\n",
    "\n",
    "# 2. Customer Segmentation Model and Scaler\n",
    "with open('models/segmentation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "    \n",
    "with open('models/segmentation_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_segment, f)\n",
    "\n",
    "# 3. Churn Prediction Model and Scaler\n",
    "with open('models/churn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_churn, f)\n",
    "    \n",
    "with open('models/churn_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_churn, f)\n",
    "\n",
    "# 4. Sales Forecasting Model\n",
    "with open('models/sales_forecast_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_forecast, f)\n",
    "\n",
    "# 5. Save feature names and model metadata\n",
    "model_metadata = {\n",
    "    'clv_features': clv_features,\n",
    "    'churn_features': churn_features,\n",
    "    'forecast_features': forecast_features,\n",
    "    'segmentation_features': segmentation_features,\n",
    "    'segment_mapping': segment_mapping,\n",
    "    'model_performance': {\n",
    "        'clv_r2': clv_r2,\n",
    "        'clv_mae': clv_mae,\n",
    "        'churn_accuracy': churn_accuracy,\n",
    "        'forecast_r2': forecast_r2,\n",
    "        'forecast_mae': forecast_mae\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(model_metadata, f)\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "for file in os.listdir('models'):\n",
    "    print(f\"- models/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f47900",
   "metadata": {},
   "source": [
    "## 8. Create Streamlit Application\n",
    "\n",
    "Build a comprehensive Streamlit web application that loads the pkl files and provides interfaces for all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit Application Code\n",
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"AdventureWorks Analytics\",\n",
    "    page_icon=\"ðŸ“Š\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Load models and metadata\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    \"\"\"Load all trained models and preprocessors\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Load CLV model\n",
    "    with open('models/clv_model.pkl', 'rb') as f:\n",
    "        models['clv_model'] = pickle.load(f)\n",
    "    with open('models/clv_scaler.pkl', 'rb') as f:\n",
    "        models['clv_scaler'] = pickle.load(f)\n",
    "    \n",
    "    # Load segmentation model\n",
    "    with open('models/segmentation_model.pkl', 'rb') as f:\n",
    "        models['segmentation_model'] = pickle.load(f)\n",
    "    with open('models/segmentation_scaler.pkl', 'rb') as f:\n",
    "        models['segmentation_scaler'] = pickle.load(f)\n",
    "    \n",
    "    # Load churn model\n",
    "    with open('models/churn_model.pkl', 'rb') as f:\n",
    "        models['churn_model'] = pickle.load(f)\n",
    "    with open('models/churn_scaler.pkl', 'rb') as f:\n",
    "        models['churn_scaler'] = pickle.load(f)\n",
    "    \n",
    "    # Load sales forecast model\n",
    "    with open('models/sales_forecast_model.pkl', 'rb') as f:\n",
    "        models['sales_forecast_model'] = pickle.load(f)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open('models/model_metadata.pkl', 'rb') as f:\n",
    "        models['metadata'] = pickle.load(f)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Main application\n",
    "def main():\n",
    "    st.title(\"ðŸš€ AdventureWorks Predictive Analytics Dashboard\")\n",
    "    st.markdown(\"Comprehensive ML-powered business insights and predictions\")\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models()\n",
    "    \n",
    "    # Sidebar navigation\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    page = st.sidebar.selectbox(\"Choose Analysis\", [\n",
    "        \"ðŸ“Š Dashboard Overview\",\n",
    "        \"ðŸ’° Customer Lifetime Value\",\n",
    "        \"ðŸ‘¥ Customer Segmentation\", \n",
    "        \"âš ï¸ Churn Prediction\",\n",
    "        \"ðŸ“ˆ Sales Forecasting\"\n",
    "    ])\n",
    "    \n",
    "    # Dashboard Overview\n",
    "    if page == \"ðŸ“Š Dashboard Overview\":\n",
    "        st.header(\"Model Performance Overview\")\n",
    "        \n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        perf = models['metadata']['model_performance']\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"CLV Model RÂ²\", f\"{perf['clv_r2']:.3f}\")\n",
    "        with col2:\n",
    "            st.metric(\"Churn Accuracy\", f\"{perf['churn_accuracy']:.3f}\")\n",
    "        with col3:\n",
    "            st.metric(\"Sales Forecast RÂ²\", f\"{perf['forecast_r2']:.3f}\")\n",
    "        with col4:\n",
    "            st.metric(\"Total Models\", \"4\")\n",
    "        \n",
    "        st.markdown(\"\"\"\n",
    "        ### Available Predictions:\n",
    "        - **Customer Lifetime Value**: Predict future customer worth\n",
    "        - **Customer Segmentation**: Identify customer groups\n",
    "        - **Churn Prediction**: Identify at-risk customers\n",
    "        - **Sales Forecasting**: Predict future sales trends\n",
    "        \"\"\")\n",
    "    \n",
    "    # Customer Lifetime Value\n",
    "    elif page == \"ðŸ’° Customer Lifetime Value\":\n",
    "        st.header(\"Customer Lifetime Value Prediction\")\n",
    "        \n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Customer Information\")\n",
    "            annual_income = st.number_input(\"Annual Income ($)\", min_value=0, max_value=200000, value=50000)\n",
    "            total_children = st.number_input(\"Number of Children\", min_value=0, max_value=10, value=1)\n",
    "            age = st.number_input(\"Age\", min_value=18, max_value=100, value=35)\n",
    "            gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "            marital_status = st.selectbox(\"Marital Status\", [\"Married\", \"Single\"])\n",
    "            education = st.selectbox(\"Education Level\", [\"High School\", \"Bachelors\", \"Graduate Degree\"])\n",
    "            total_orders = st.number_input(\"Historical Orders\", min_value=0, max_value=100, value=5)\n",
    "            total_quantity = st.number_input(\"Total Items Purchased\", min_value=0, max_value=1000, value=25)\n",
    "            avg_order_size = st.number_input(\"Average Order Size\", min_value=0.0, max_value=50.0, value=5.0)\n",
    "            lifespan_days = st.number_input(\"Customer Lifespan (days)\", min_value=0, max_value=3650, value=365)\n",
    "            \n",
    "            if st.button(\"Predict CLV\"):\n",
    "                # Prepare features\n",
    "                features = np.array([[\n",
    "                    annual_income, total_children, age, 1 if gender == \"Male\" else 0,\n",
    "                    1 if marital_status == \"Married\" else 0, 2 if education == \"Bachelors\" else (3 if education == \"Graduate Degree\" else 1),\n",
    "                    total_orders, total_quantity, avg_order_size, lifespan_days\n",
    "                ]])\n",
    "                \n",
    "                # Scale and predict\n",
    "                features_scaled = models['clv_scaler'].transform(features)\n",
    "                clv_prediction = models['clv_model'].predict(features_scaled)[0]\n",
    "                \n",
    "                with col2:\n",
    "                    st.subheader(\"CLV Prediction Results\")\n",
    "                    st.success(f\"Predicted Customer Lifetime Value: ${clv_prediction:,.2f}\")\n",
    "                    \n",
    "                    # CLV category\n",
    "                    if clv_prediction > 1000:\n",
    "                        category = \"High Value Customer\"\n",
    "                        color = \"green\"\n",
    "                    elif clv_prediction > 500:\n",
    "                        category = \"Medium Value Customer\"\n",
    "                        color = \"orange\"\n",
    "                    else:\n",
    "                        category = \"Low Value Customer\" \n",
    "                        color = \"red\"\n",
    "                    \n",
    "                    st.markdown(f\"Customer Category: :{color}[{category}]\")\n",
    "    \n",
    "    # Customer Segmentation\n",
    "    elif page == \"ðŸ‘¥ Customer Segmentation\":\n",
    "        st.header(\"Customer Segmentation Analysis\")\n",
    "        \n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Customer Profile\")\n",
    "            annual_income = st.number_input(\"Annual Income\", min_value=0, max_value=200000, value=50000)\n",
    "            total_children = st.number_input(\"Children\", min_value=0, max_value=10, value=1)\n",
    "            age = st.number_input(\"Age\", min_value=18, max_value=100, value=35)\n",
    "            total_orders = st.number_input(\"Total Orders\", min_value=0, max_value=100, value=5)\n",
    "            total_quantity = st.number_input(\"Total Quantity\", min_value=0, max_value=1000, value=25)\n",
    "            avg_order_size = st.number_input(\"Avg Order Size\", min_value=0.0, max_value=50.0, value=5.0)\n",
    "            recency_days = st.number_input(\"Days Since Last Order\", min_value=0, max_value=1000, value=30)\n",
    "            \n",
    "            if st.button(\"Identify Segment\"):\n",
    "                # Prepare features\n",
    "                features = np.array([[annual_income, total_children, age, total_orders, total_quantity, avg_order_size, recency_days]])\n",
    "                \n",
    "                # Scale and predict\n",
    "                features_scaled = models['segmentation_scaler'].transform(features)\n",
    "                segment = models['segmentation_model'].predict(features_scaled)[0]\n",
    "                segment_name = models['metadata']['segment_mapping'][segment]\n",
    "                \n",
    "                with col2:\n",
    "                    st.subheader(\"Segmentation Results\")\n",
    "                    st.success(f\"Customer Segment: {segment_name}\")\n",
    "                    \n",
    "                    # Segment characteristics\n",
    "                    segment_info = {\n",
    "                        'Low Value': 'Infrequent buyers with low spend',\n",
    "                        'Occasional': 'Moderate buyers with average spend', \n",
    "                        'Regular': 'Consistent buyers with good spend',\n",
    "                        'High Value': 'Frequent buyers with high spend',\n",
    "                        'Premium': 'Top-tier customers with highest value'\n",
    "                    }\n",
    "                    \n",
    "                    st.info(segment_info.get(segment_name, \"Unknown segment\"))\n",
    "    \n",
    "    # Churn Prediction\n",
    "    elif page == \"âš ï¸ Churn Prediction\":\n",
    "        st.header(\"Customer Churn Prediction\")\n",
    "        \n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Customer Data\")\n",
    "            annual_income = st.number_input(\"Annual Income\", min_value=0, value=50000)\n",
    "            total_children = st.number_input(\"Children\", min_value=0, value=1)\n",
    "            age = st.number_input(\"Age\", min_value=18, value=35)\n",
    "            gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "            marital_status = st.selectbox(\"Marital Status\", [\"Married\", \"Single\"])\n",
    "            education = st.selectbox(\"Education\", [\"High School\", \"Bachelors\", \"Graduate Degree\"])\n",
    "            total_orders = st.number_input(\"Total Orders\", min_value=0, value=5)\n",
    "            avg_order_size = st.number_input(\"Avg Order Size\", min_value=0.0, value=5.0)\n",
    "            lifespan_days = st.number_input(\"Customer Lifespan (days)\", min_value=0, value=365)\n",
    "            recency_days = st.number_input(\"Days Since Last Order\", min_value=0, value=30)\n",
    "            \n",
    "            if st.button(\"Predict Churn Risk\"):\n",
    "                # Prepare features\n",
    "                features = np.array([[\n",
    "                    annual_income, total_children, age, 1 if gender == \"Male\" else 0,\n",
    "                    1 if marital_status == \"Married\" else 0, 2 if education == \"Bachelors\" else (3 if education == \"Graduate Degree\" else 1),\n",
    "                    total_orders, avg_order_size, lifespan_days, recency_days\n",
    "                ]])\n",
    "                \n",
    "                # Scale and predict\n",
    "                features_scaled = models['churn_scaler'].transform(features)\n",
    "                churn_prob = models['churn_model'].predict_proba(features_scaled)[0][1]\n",
    "                churn_prediction = models['churn_model'].predict(features_scaled)[0]\n",
    "                \n",
    "                with col2:\n",
    "                    st.subheader(\"Churn Prediction Results\")\n",
    "                    \n",
    "                    if churn_prediction == 1:\n",
    "                        st.error(f\"High Churn Risk: {churn_prob:.2%} probability\")\n",
    "                        st.markdown(\"**Recommended Actions:**\")\n",
    "                        st.markdown(\"- Send retention offers\")\n",
    "                        st.markdown(\"- Personalized engagement campaign\")\n",
    "                        st.markdown(\"- Customer service outreach\")\n",
    "                    else:\n",
    "                        st.success(f\"Low Churn Risk: {churn_prob:.2%} probability\")\n",
    "                        st.markdown(\"**Customer Status:** Likely to remain active\")\n",
    "                    \n",
    "                    # Probability gauge\n",
    "                    fig = go.Figure(go.Indicator(\n",
    "                        mode = \"gauge+number\",\n",
    "                        value = churn_prob * 100,\n",
    "                        domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "                        title = {'text': \"Churn Probability (%)\"},\n",
    "                        gauge = {\n",
    "                            'axis': {'range': [None, 100]},\n",
    "                            'bar': {'color': \"red\" if churn_prob > 0.5 else \"green\"},\n",
    "                            'steps': [\n",
    "                                {'range': [0, 50], 'color': \"lightgray\"},\n",
    "                                {'range': [50, 100], 'color': \"gray\"}],\n",
    "                            'threshold': {\n",
    "                                'line': {'color': \"red\", 'width': 4},\n",
    "                                'thickness': 0.75,\n",
    "                                'value': 90}}))\n",
    "                    st.plotly_chart(fig)\n",
    "    \n",
    "    # Sales Forecasting\n",
    "    elif page == \"ðŸ“ˆ Sales Forecasting\":\n",
    "        st.header(\"Sales Forecasting\")\n",
    "        \n",
    "        st.subheader(\"Forecast Parameters\")\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            month = st.selectbox(\"Month\", range(1, 13), index=5)\n",
    "        with col2:\n",
    "            day = st.selectbox(\"Day\", range(1, 32), index=14)\n",
    "        with col3:\n",
    "            dayofweek = st.selectbox(\"Day of Week\", range(0, 7), index=2)\n",
    "        with col4:\n",
    "            quarter = st.selectbox(\"Quarter\", [1, 2, 3, 4], index=1)\n",
    "        \n",
    "        col5, col6 = st.columns(2)\n",
    "        with col5:\n",
    "            unique_customers = st.number_input(\"Expected Unique Customers\", min_value=1, value=100)\n",
    "            unique_products = st.number_input(\"Expected Unique Products\", min_value=1, value=50)\n",
    "        \n",
    "        with col6:\n",
    "            lag_1 = st.number_input(\"Previous Day Sales\", min_value=0, value=150)\n",
    "            lag_7 = st.number_input(\"Same Day Last Week Sales\", min_value=0, value=140)\n",
    "        \n",
    "        rolling_mean_7 = st.number_input(\"7-Day Average Sales\", min_value=0, value=145)\n",
    "        \n",
    "        if st.button(\"Generate Forecast\"):\n",
    "            # Prepare features\n",
    "            features = np.array([[month, day, dayofweek, quarter, unique_customers, unique_products, lag_1, lag_7, rolling_mean_7]])\n",
    "            \n",
    "            # Predict\n",
    "            forecast = models['sales_forecast_model'].predict(features)[0]\n",
    "            \n",
    "            st.success(f\"Predicted Sales Quantity: {forecast:.0f} units\")\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(range(1, 8)),\n",
    "                y=[lag_7, 135, 142, 138, 141, lag_1, forecast],\n",
    "                mode='lines+markers',\n",
    "                name='Sales Trend',\n",
    "                line=dict(color='blue', width=3),\n",
    "                marker=dict(size=8)\n",
    "            ))\n",
    "            fig.update_layout(\n",
    "                title='Sales Forecast Visualization',\n",
    "                xaxis_title='Time Period',\n",
    "                yaxis_title='Sales Quantity',\n",
    "                showlegend=True\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save Streamlit application\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"Streamlit application created successfully!\")\n",
    "print(\"File saved as: streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8ed86",
   "metadata": {},
   "source": [
    "## 9. Test the Streamlit App\n",
    "\n",
    "Instructions to run the Streamlit application locally and test its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35209b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for running the Streamlit app\n",
    "instructions = \"\"\"\n",
    "TO RUN THE STREAMLIT APPLICATION:\n",
    "\n",
    "1. Open terminal/command prompt\n",
    "2. Navigate to the project directory\n",
    "3. Run the following command:\n",
    "   \n",
    "   streamlit run streamlit_app.py\n",
    "\n",
    "4. The app will open in your browser at http://localhost:8501\n",
    "\n",
    "FEATURES OF THE APP:\n",
    "- Dashboard Overview with model performance metrics\n",
    "- Customer Lifetime Value prediction with interactive inputs\n",
    "- Customer Segmentation analysis\n",
    "- Churn Prediction with probability visualization  \n",
    "- Sales Forecasting with trend charts\n",
    "\n",
    "TESTING CHECKLIST:\n",
    "â–¡ All models load successfully\n",
    "â–¡ CLV predictions work with different input values\n",
    "â–¡ Customer segmentation identifies correct segments\n",
    "â–¡ Churn prediction shows probability gauge\n",
    "â–¡ Sales forecasting generates reasonable predictions\n",
    "â–¡ All visualizations render correctly\n",
    "\n",
    "The app uses the trained PKL files for real-time predictions!\n",
    "\"\"\"\n",
    "\n",
    "print(instructions)\n",
    "\n",
    "# Create a requirements.txt file for easy deployment\n",
    "requirements = \"\"\"\n",
    "streamlit\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "plotly\n",
    "pickle-mixin\n",
    "\"\"\"\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"\\nrequirements.txt file created for easy deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed6d14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully created a comprehensive machine learning pipeline for AdventureWorks:\n",
    "\n",
    "### Models Created & Saved:\n",
    "1. **Customer Lifetime Value (CLV)** - Random Forest Regression\n",
    "2. **Customer Segmentation** - KMeans Clustering (5 segments)\n",
    "3. **Customer Churn Prediction** - Random Forest Classification\n",
    "4. **Sales Forecasting** - Random Forest Regression\n",
    "\n",
    "### PKL Files Generated:\n",
    "- `clv_model.pkl` & `clv_scaler.pkl`\n",
    "- `segmentation_model.pkl` & `segmentation_scaler.pkl` \n",
    "- `churn_model.pkl` & `churn_scaler.pkl`\n",
    "- `sales_forecast_model.pkl`\n",
    "- `model_metadata.pkl`\n",
    "\n",
    "### Streamlit Application:\n",
    "- Interactive web interface for all predictions\n",
    "- Real-time model inference using PKL files\n",
    "- Professional dashboard with visualizations\n",
    "- Business-ready insights and recommendations\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run `streamlit run streamlit_app.py` to launch the application\n",
    "2. Test all prediction features\n",
    "3. Deploy to cloud platform if needed\n",
    "4. Integrate with business systems for automated insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
