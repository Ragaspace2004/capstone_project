{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756413cd",
   "metadata": {},
   "source": [
    "# AdventureWorks Predictive Analytics - ML Models & PKL Export\n",
    "\n",
    "This notebook builds comprehensive machine learning models for AdventureWorks dataset including:\n",
    "- Sales Forecasting\n",
    "- Customer Lifetime Value (CLV)  \n",
    "- Customer Segmentation\n",
    "- Customer Churn Prediction\n",
    "- Demand Forecasting\n",
    "\n",
    "All trained models will be saved as `.pkl` files for use in Streamlit application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65b05d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including pandas, numpy, scikit-learn, pickle, and streamlit dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb92579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Setting up AdventureWorks ML Environment...\n",
      "============================================================\n",
      "📦 Installing packages with UV...\n",
      "✅ All packages installed successfully!\n",
      "\n",
      "🔧 Environment Setup Complete!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "🔍 Checking package versions...\n",
      "✅ numpy: 2.3.2\n",
      "✅ pandas: 2.3.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m     install_packages()\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m     \u001b[43mcheck_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 Ready to run machine learning models!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestart kernel if you encounter any import issues.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 110\u001b[0m, in \u001b[0;36mcheck_environment\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m package_import, package_name \u001b[38;5;129;01min\u001b[39;00m packages_to_check\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage_import\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m         version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\GANES\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:994\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\__init__.py:87\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     )\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     90\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compress, islice\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\sparse\\__init__.py:297\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dok\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\sparse\\_lil.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexMixin, INT_TYPES, _broadcast_arrays\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (getdtype, isshape, isscalarlike, upcast_scalar,\n\u001b[0;32m     16\u001b[0m                        check_shape, check_reshape_kwargs)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _csparsetools\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_lil_base\u001b[39;00m(_spbase, IndexMixin):\n\u001b[0;32m     21\u001b[0m     _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlil\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mscipy\\\\sparse\\\\_csparsetools.pyx:1\u001b[0m, in \u001b[0;36minit _csparsetools\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE PACKAGE INSTALLATION & ENVIRONMENT SETUP\n",
    "# Run this cell first to install all required packages and resolve compatibility issues\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def fix_numpy_compatibility():\n",
    "    \"\"\"Fix NumPy-SciPy binary incompatibility by installing compatible versions\"\"\"\n",
    "    \n",
    "    print(\"🔧 FIXING NUMPY-SCIPY COMPATIBILITY ISSUE...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, uninstall problematic packages\n",
    "    packages_to_uninstall = [\"numpy\", \"scipy\", \"scikit-learn\", \"seaborn\"]\n",
    "    \n",
    "    print(\"\uddd1️  Uninstalling incompatible packages...\")\n",
    "    for package in packages_to_uninstall:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", package, \"-y\"], \n",
    "                         capture_output=True, check=False)\n",
    "            print(f\"   Uninstalled: {package}\")\n",
    "        except:\n",
    "            print(f\"   Skipped: {package}\")\n",
    "    \n",
    "    # Install compatible versions in specific order\n",
    "    print(\"\\n📦 Installing compatible package versions...\")\n",
    "    \n",
    "    compatible_packages = [\n",
    "        # Core numerical packages - MUST be compatible versions\n",
    "        \"numpy==1.26.4\",  # Last stable 1.x version - widely compatible\n",
    "        \"scipy==1.11.4\",  # Compatible with NumPy 1.26.x\n",
    "        \"scikit-learn==1.4.0\",  # Compatible with above versions\n",
    "        \n",
    "        # Data manipulation\n",
    "        \"pandas>=2.0.0,<2.5.0\",\n",
    "        \n",
    "        # Visualization (install after numpy/scipy)\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"seaborn>=0.12.0\",\n",
    "        \"plotly>=5.15.0\"\n",
    "    ]\n",
    "    \n",
    "    # Install packages one by one to catch errors\n",
    "    for package in compatible_packages:\n",
    "        try:\n",
    "            print(f\"Installing {package}...\")\n",
    "            result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"✅ {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Failed to install {package}: {e}\")\n",
    "            print(f\"   Error output: {e.stderr}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def install_remaining_packages():\n",
    "    \"\"\"Install remaining packages after fixing core compatibility\"\"\"\n",
    "    \n",
    "    print(\"\\n📦 Installing additional packages...\")\n",
    "    \n",
    "    # Database and serialization\n",
    "    database_packages = [\n",
    "        \"sqlalchemy>=2.0.0\",\n",
    "        \"pymysql>=1.0.0\"\n",
    "    ]\n",
    "    \n",
    "    # Machine Learning extras (install after scikit-learn)\n",
    "    ml_packages = [\n",
    "        \"xgboost>=1.7.0\",\n",
    "        \"lightgbm>=4.0.0\"\n",
    "    ]\n",
    "    \n",
    "    # Web application\n",
    "    web_packages = [\n",
    "        \"streamlit>=1.25.0\",\n",
    "        \"streamlit-option-menu>=0.3.0\"\n",
    "    ]\n",
    "    \n",
    "    # Utility packages\n",
    "    utility_packages = [\n",
    "        \"jupyter>=1.0.0\",\n",
    "        \"ipywidgets>=8.0.0\",\n",
    "        \"tqdm>=4.65.0\"\n",
    "    ]\n",
    "    \n",
    "    # TensorFlow (optional - known to have compatibility issues)\n",
    "    optional_packages = [\n",
    "        \"tensorflow>=2.13.0,<2.16.0\"  # Version range that works with NumPy 1.26\n",
    "    ]\n",
    "    \n",
    "    all_additional = database_packages + ml_packages + web_packages + utility_packages\n",
    "    \n",
    "    # Install additional packages\n",
    "    for package in all_additional:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                         capture_output=True, check=True)\n",
    "            print(f\"✅ {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"⚠️  Skipped: {package}\")\n",
    "    \n",
    "    # Try TensorFlow separately (may fail on some systems)\n",
    "    print(\"\\n🤖 Installing TensorFlow (optional)...\")\n",
    "    for package in optional_packages:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                         capture_output=True, check=True)\n",
    "            print(f\"✅ {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"⚠️  TensorFlow installation failed - deep learning features will be disabled\")\n",
    "\n",
    "def check_compatibility():\n",
    "    \"\"\"Check if packages are properly installed and compatible\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 Verifying package compatibility...\")\n",
    "    \n",
    "    try:\n",
    "        import numpy as np\n",
    "        print(f\"✅ NumPy: {np.__version__}\")\n",
    "        \n",
    "        import scipy\n",
    "        print(f\"✅ SciPy: {scipy.__version__}\")\n",
    "        \n",
    "        import sklearn\n",
    "        print(f\"✅ Scikit-learn: {sklearn.__version__}\")\n",
    "        \n",
    "        import pandas as pd\n",
    "        print(f\"✅ Pandas: {pd.__version__}\")\n",
    "        \n",
    "        # Test critical imports\n",
    "        from scipy import stats\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        print(\"✅ Critical imports working\")\n",
    "        \n",
    "        # Check NumPy version compatibility\n",
    "        np_version = tuple(map(int, np.__version__.split('.')[:2]))\n",
    "        if np_version == (1, 26):\n",
    "            print(\"✅ NumPy version is optimal for compatibility\")\n",
    "        else:\n",
    "            print(f\"⚠️  NumPy version {np.__version__} - may have some compatibility issues\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Compatibility check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main execution\n",
    "print(\"🚀 FIXING ADVENTUREWORKS ML ENVIRONMENT...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Fix core compatibility\n",
    "if fix_numpy_compatibility():\n",
    "    print(\"\\n✅ Core packages installed successfully!\")\n",
    "    \n",
    "    # Step 2: Install additional packages\n",
    "    install_remaining_packages()\n",
    "    \n",
    "    # Step 3: Verify everything works\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if check_compatibility():\n",
    "        print(\"\\n🎉 Environment setup completed successfully!\")\n",
    "        print(\"All packages are compatible and ready to use.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Some compatibility issues remain.\")\n",
    "        print(\"Try restarting the kernel and running this cell again.\")\n",
    "else:\n",
    "    print(\"\\n❌ Failed to install core packages.\")\n",
    "    print(\"Manual installation may be required.\")\n",
    "\n",
    "print(\"\\n🚀 Ready to run machine learning models!\")\n",
    "print(\"📝 Note: Restart kernel before proceeding to imports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d57a19",
   "metadata": {},
   "source": [
    "## 0. 🔧 Environment Setup & Compatibility Fix\n",
    "\n",
    "**IMPORTANT: NumPy-SciPy Compatibility Issue Detected!**\n",
    "\n",
    "You're experiencing a binary incompatibility between NumPy 2.3.2 and SciPy. This is a common issue when NumPy gets a major version update.\n",
    "\n",
    "### 🚨 **Required Steps:**\n",
    "\n",
    "1. **Run the cell below** - This will automatically fix the compatibility issue\n",
    "2. **Restart your kernel** after the cell completes successfully  \n",
    "3. **Proceed to the import cell** - All packages will work correctly\n",
    "\n",
    "### 🔧 **What the fix does:**\n",
    "- Uninstalls incompatible NumPy 2.3.2 and related packages\n",
    "- Installs NumPy 1.26.4 (stable, widely compatible version)\n",
    "- Installs compatible versions of SciPy, scikit-learn, etc.\n",
    "- Verifies all packages work together\n",
    "\n",
    "### ⚡ **Quick Manual Fix (Alternative):**\n",
    "If you prefer to fix manually via terminal:\n",
    "```bash\n",
    "pip uninstall numpy scipy scikit-learn seaborn -y\n",
    "pip install numpy==1.26.4 scipy==1.11.4 scikit-learn==1.4.0 seaborn>=0.12.0\n",
    "```\n",
    "\n",
    "**Run the cell below now to automatically fix everything:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412f2181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Configuring environment...\n",
      "✅ NumPy 2.3.2 | Pandas 2.3.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Visualization Libraries\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\seaborn\\relational.py:21\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     15\u001b[0m     _default_color,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     _scatter_legend_artist,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m groupby_apply_include_groups\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator, WeightedAggregator\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\seaborn\\_statistics.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     33\u001b[0m     _no_scipy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\stats\\__init__.py:605\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m \n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    604\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\stats\\_stats_py.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper, _get_nan,\n\u001b[0;32m     40\u001b[0m                               rng_integers, _rename_parameter, _contains_nan,\n\u001b[0;32m     41\u001b[0m                               AxisError)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\spatial\\__init__.py:110\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries with Compatibility Handling\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Suppress specific compatibility warnings\n",
    "warnings.filterwarnings('ignore', message='A NumPy version.*is required for this version of SciPy')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"🔧 Configuring environment...\")\n",
    "\n",
    "try:\n",
    "    # Core Data Science Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(f\"✅ NumPy {np.__version__} | Pandas {pd.__version__}\")\n",
    "    \n",
    "    # Visualization Libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    print(\"✅ Visualization libraries loaded\")\n",
    "    \n",
    "    # Database Connection\n",
    "    from sqlalchemy import create_engine\n",
    "    print(\"✅ Database connectivity ready\")\n",
    "    \n",
    "    # Serialization\n",
    "    import pickle\n",
    "    print(\"✅ Model serialization ready\")\n",
    "    \n",
    "    # Machine Learning Libraries\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    print(\"✅ Scikit-learn libraries loaded\")\n",
    "    \n",
    "    # Deep Learning (Optional - will not fail if not available)\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "        print(f\"✅ TensorFlow {tf.__version__} loaded\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️  TensorFlow not available - deep learning features disabled\")\n",
    "        tf = None\n",
    "\n",
    "    print(\"\\n🎉 All essential libraries imported successfully!\")\n",
    "    \n",
    "    # Environment compatibility check\n",
    "    print(f\"\\n📊 Environment Info:\")\n",
    "    print(f\"   Python: {sys.version.split()[0]}\")\n",
    "    print(f\"   NumPy: {np.__version__}\")\n",
    "    print(f\"   Pandas: {pd.__version__}\")\n",
    "    \n",
    "    # Check for compatibility issues\n",
    "    np_version = tuple(map(int, np.__version__.split('.')[:3]))\n",
    "    if np_version >= (2, 0, 0):\n",
    "        print(\"⚠️  Note: NumPy 2.x detected - some packages may show compatibility warnings\")\n",
    "        print(\"   This is expected and won't affect functionality\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import Error: {e}\")\n",
    "    print(\"Please run the package installation cell above first!\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeef266",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "Load the cleaned AdventureWorks dataset from MySQL and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0852d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m PASSWORD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m DATABASE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madventureworks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_engine\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmysql+pymysql://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUSER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPASSWORD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATABASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?charset=utf8mb4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load all cleaned datasets\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_engine' is not defined"
     ]
    }
   ],
   "source": [
    "# Database connection\n",
    "# Import required libraries for database connectivity\n",
    "try:\n",
    "    from sqlalchemy import create_engine\n",
    "    import pandas as pd\n",
    "    print(\"✅ Database libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Please run the package installation and import cells first!\")\n",
    "    raise\n",
    "\n",
    "HOST = \"localhost\"\n",
    "PORT = 3306\n",
    "USER = \"root\"\n",
    "PASSWORD = \"root\"\n",
    "DATABASE = \"adventureworks\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(f\"mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4\")\n",
    "    print(\"✅ Database engine created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Database connection error: {e}\")\n",
    "    print(\"Please ensure MySQL server is running and credentials are correct\")\n",
    "    raise\n",
    "\n",
    "# Load all cleaned datasets\n",
    "print(\"\\n📊 Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    # Load customer data\n",
    "    customers_df = pd.read_sql(\"SELECT * FROM customer_trans\", engine)\n",
    "    print(f\"✅ Customers: {len(customers_df)} records\")\n",
    "\n",
    "    # Load sales data\n",
    "    sales_df = pd.read_sql(\"SELECT * FROM sales_trans\", engine)\n",
    "    print(f\"✅ Sales: {len(sales_df)} records\")\n",
    "\n",
    "    # Load products data\n",
    "    products_df = pd.read_sql(\"SELECT * FROM products_trans\", engine)\n",
    "    print(f\"✅ Products: {len(products_df)} records\")\n",
    "\n",
    "    # Load territories data\n",
    "    territories_df = pd.read_sql(\"SELECT * FROM territories_trans\", engine)\n",
    "    print(f\"✅ Territories: {len(territories_df)} records\")\n",
    "\n",
    "    print(f\"\\n🎉 Dataset loading complete!\")\n",
    "    print(f\"Total records loaded: {len(customers_df) + len(sales_df) + len(products_df) + len(territories_df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    print(\"Please check if all tables exist in the database\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb85024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "print(\"CUSTOMER DATA STRUCTURE:\")\n",
    "print(customers_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(customers_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SALES DATA STRUCTURE:\")\n",
    "print(sales_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a133b2f",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Clean the data, handle missing values, encode categorical variables, and create features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Function\n",
    "def preprocess_data():\n",
    "    \"\"\"Comprehensive data preprocessing for all models\"\"\"\n",
    "    \n",
    "    # Convert date columns\n",
    "    sales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\n",
    "    customers_df['birth_date'] = pd.to_datetime(customers_df['birth_date'])\n",
    "    \n",
    "    # Create customer age\n",
    "    current_date = pd.Timestamp.now()\n",
    "    customers_df['age'] = (current_date - customers_df['birth_date']).dt.days // 365\n",
    "    \n",
    "    # Handle missing values\n",
    "    customers_df['annual_income'].fillna(customers_df['annual_income'].median(), inplace=True)\n",
    "    customers_df['total_children'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_gender = LabelEncoder()\n",
    "    le_marital = LabelEncoder()\n",
    "    le_education = LabelEncoder()\n",
    "    \n",
    "    customers_df['gender_encoded'] = le_gender.fit_transform(customers_df['gender'].fillna('Unknown'))\n",
    "    customers_df['marital_status_encoded'] = le_marital.fit_transform(customers_df['marital_status'].fillna('Unknown'))\n",
    "    customers_df['education_encoded'] = le_education.fit_transform(customers_df['education_level'].fillna('Unknown'))\n",
    "    \n",
    "    # Create aggregated customer features\n",
    "    customer_sales = sales_df.groupby('customer_id').agg({\n",
    "        'order_quantity': ['count', 'sum', 'mean'],\n",
    "        'order_date': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_sales.columns = ['customer_id', 'total_orders', 'total_quantity', 'avg_order_size', 'first_order', 'last_order']\n",
    "    \n",
    "    # Calculate customer metrics\n",
    "    customer_sales['customer_lifespan_days'] = (customer_sales['last_order'] - customer_sales['first_order']).dt.days\n",
    "    customer_sales['recency_days'] = (current_date - customer_sales['last_order']).dt.days\n",
    "    \n",
    "    # Merge customer and sales data\n",
    "    customer_features = customers_df.merge(customer_sales, on='customer_id', how='left')\n",
    "    customer_features.fillna(0, inplace=True)\n",
    "    \n",
    "    print(\"Data preprocessing completed!\")\n",
    "    return customer_features\n",
    "\n",
    "# Execute preprocessing\n",
    "customer_features = preprocess_data()\n",
    "print(f\"Final dataset shape: {customer_features.shape}\")\n",
    "print(customer_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b961b6a",
   "metadata": {},
   "source": [
    "## 4. Split Data into Training and Testing Sets\n",
    "\n",
    "Prepare data splits for different modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for different models\n",
    "\n",
    "# 1. Customer Lifetime Value (CLV) Dataset\n",
    "clv_features = ['annual_income', 'total_children', 'age', 'gender_encoded', \n",
    "               'marital_status_encoded', 'education_encoded', 'total_orders', \n",
    "               'total_quantity', 'avg_order_size', 'customer_lifespan_days']\n",
    "\n",
    "# Create CLV target (proxy using total value)\n",
    "customer_features['clv_target'] = (customer_features['total_quantity'] * \n",
    "                                  customer_features['avg_order_size'] * \n",
    "                                  (1 + customer_features['total_orders'] / 100))\n",
    "\n",
    "# Remove outliers for CLV\n",
    "clv_data = customer_features[customer_features['total_orders'] > 0].copy()\n",
    "Q1 = clv_data['clv_target'].quantile(0.25)\n",
    "Q3 = clv_data['clv_target'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "clv_data = clv_data[(clv_data['clv_target'] >= Q1 - 1.5*IQR) & \n",
    "                    (clv_data['clv_target'] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "X_clv = clv_data[clv_features]\n",
    "y_clv = clv_data['clv_target']\n",
    "\n",
    "X_clv_train, X_clv_test, y_clv_train, y_clv_test = train_test_split(X_clv, y_clv, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"CLV Dataset - Train: {X_clv_train.shape}, Test: {X_clv_test.shape}\")\n",
    "\n",
    "# 2. Customer Segmentation Dataset (unsupervised)\n",
    "segmentation_features = ['annual_income', 'total_children', 'age', 'total_orders', \n",
    "                        'total_quantity', 'avg_order_size', 'recency_days']\n",
    "X_segment = clv_data[segmentation_features]\n",
    "\n",
    "print(f\"Segmentation Dataset: {X_segment.shape}\")\n",
    "\n",
    "# 3. Churn Prediction Dataset\n",
    "# Define churn as customers with no orders in last 180 days\n",
    "customer_features['is_churned'] = (customer_features['recency_days'] > 180).astype(int)\n",
    "churn_data = customer_features[customer_features['total_orders'] > 0].copy()\n",
    "\n",
    "churn_features = ['annual_income', 'total_children', 'age', 'gender_encoded', \n",
    "                 'marital_status_encoded', 'education_encoded', 'total_orders', \n",
    "                 'avg_order_size', 'customer_lifespan_days', 'recency_days']\n",
    "\n",
    "X_churn = churn_data[churn_features]\n",
    "y_churn = churn_data['is_churned']\n",
    "\n",
    "X_churn_train, X_churn_test, y_churn_train, y_churn_test = train_test_split(X_churn, y_churn, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Churn Dataset - Train: {X_churn_train.shape}, Test: {X_churn_test.shape}\")\n",
    "print(f\"Churn Distribution: {y_churn.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee4361",
   "metadata": {},
   "source": [
    "## 5. Train Machine Learning Models\n",
    "\n",
    "Train multiple ML models including Random Forest, clustering, and classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60711e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CUSTOMER LIFETIME VALUE (CLV) MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CUSTOMER LIFETIME VALUE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for CLV\n",
    "scaler_clv = StandardScaler()\n",
    "X_clv_train_scaled = scaler_clv.fit_transform(X_clv_train)\n",
    "X_clv_test_scaled = scaler_clv.transform(X_clv_test)\n",
    "\n",
    "# Train Random Forest for CLV\n",
    "rf_clv = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_clv.fit(X_clv_train_scaled, y_clv_train)\n",
    "\n",
    "# Predictions\n",
    "y_clv_pred = rf_clv.predict(X_clv_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "clv_mae = mean_absolute_error(y_clv_test, y_clv_pred)\n",
    "clv_rmse = np.sqrt(mean_squared_error(y_clv_test, y_clv_pred))\n",
    "clv_r2 = r2_score(y_clv_test, y_clv_pred)\n",
    "\n",
    "print(f\"CLV Model Performance:\")\n",
    "print(f\"MAE: {clv_mae:.2f}\")\n",
    "print(f\"RMSE: {clv_rmse:.2f}\")\n",
    "print(f\"R²: {clv_r2:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "clv_importance = pd.DataFrame({\n",
    "    'feature': clv_features,\n",
    "    'importance': rf_clv.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop CLV Features:\")\n",
    "print(clv_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774bb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CUSTOMER SEGMENTATION MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CUSTOMER SEGMENTATION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for segmentation\n",
    "scaler_segment = StandardScaler()\n",
    "X_segment_scaled = scaler_segment.fit_transform(X_segment)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "k_range = range(2, 11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_segment_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Train KMeans with optimal clusters (let's use 5)\n",
    "optimal_k = 5\n",
    "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "segment_labels = kmeans_model.fit_predict(X_segment_scaled)\n",
    "\n",
    "# Add segments to data\n",
    "X_segment['segment'] = segment_labels\n",
    "\n",
    "# Analyze segments\n",
    "segment_analysis = X_segment.groupby('segment').agg({\n",
    "    'annual_income': 'mean',\n",
    "    'total_orders': 'mean',\n",
    "    'total_quantity': 'mean',\n",
    "    'recency_days': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(f\"Customer Segments Analysis:\")\n",
    "print(segment_analysis)\n",
    "\n",
    "# Create segment labels\n",
    "segment_names = ['Low Value', 'Occasional', 'Regular', 'High Value', 'Premium']\n",
    "segment_mapping = {i: segment_names[i] for i in range(optimal_k)}\n",
    "X_segment['segment_name'] = X_segment['segment'].map(segment_mapping)\n",
    "\n",
    "print(f\"\\nSegment Distribution:\")\n",
    "print(X_segment['segment_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd089b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CUSTOMER CHURN PREDICTION MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CUSTOMER CHURN PREDICTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for churn prediction\n",
    "scaler_churn = StandardScaler()\n",
    "X_churn_train_scaled = scaler_churn.fit_transform(X_churn_train)\n",
    "X_churn_test_scaled = scaler_churn.transform(X_churn_test)\n",
    "\n",
    "# Train Random Forest for churn prediction\n",
    "rf_churn = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_churn.fit(X_churn_train_scaled, y_churn_train)\n",
    "\n",
    "# Predictions\n",
    "y_churn_pred = rf_churn.predict(X_churn_test_scaled)\n",
    "y_churn_pred_proba = rf_churn.predict_proba(X_churn_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "churn_accuracy = accuracy_score(y_churn_test, y_churn_pred)\n",
    "print(f\"Churn Model Accuracy: {churn_accuracy:.3f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_churn_test, y_churn_pred))\n",
    "\n",
    "# Feature importance\n",
    "churn_importance = pd.DataFrame({\n",
    "    'feature': churn_features,\n",
    "    'importance': rf_churn.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop Churn Prediction Features:\")\n",
    "print(churn_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SALES FORECASTING MODEL\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SALES FORECASTING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare time series data for sales forecasting\n",
    "sales_df['order_date'] = pd.to_datetime(sales_df['order_date'])\n",
    "daily_sales = sales_df.groupby('order_date').agg({\n",
    "    'order_quantity': 'sum',\n",
    "    'customer_id': 'nunique',\n",
    "    'product_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "daily_sales.columns = ['date', 'total_quantity', 'unique_customers', 'unique_products']\n",
    "\n",
    "# Create time-based features\n",
    "daily_sales['year'] = daily_sales['date'].dt.year\n",
    "daily_sales['month'] = daily_sales['date'].dt.month\n",
    "daily_sales['day'] = daily_sales['date'].dt.day\n",
    "daily_sales['dayofweek'] = daily_sales['date'].dt.dayofweek\n",
    "daily_sales['quarter'] = daily_sales['date'].dt.quarter\n",
    "\n",
    "# Create lag features\n",
    "daily_sales['lag_1'] = daily_sales['total_quantity'].shift(1)\n",
    "daily_sales['lag_7'] = daily_sales['total_quantity'].shift(7)\n",
    "daily_sales['rolling_mean_7'] = daily_sales['total_quantity'].rolling(7).mean()\n",
    "\n",
    "# Remove rows with NaN values\n",
    "daily_sales_clean = daily_sales.dropna()\n",
    "\n",
    "# Features and target for sales forecasting\n",
    "forecast_features = ['month', 'day', 'dayofweek', 'quarter', 'unique_customers', \n",
    "                    'unique_products', 'lag_1', 'lag_7', 'rolling_mean_7']\n",
    "X_forecast = daily_sales_clean[forecast_features]\n",
    "y_forecast = daily_sales_clean['total_quantity']\n",
    "\n",
    "X_forecast_train, X_forecast_test, y_forecast_train, y_forecast_test = train_test_split(\n",
    "    X_forecast, y_forecast, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Train Random Forest for sales forecasting\n",
    "rf_forecast = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_forecast.fit(X_forecast_train, y_forecast_train)\n",
    "\n",
    "# Predictions\n",
    "y_forecast_pred = rf_forecast.predict(X_forecast_test)\n",
    "\n",
    "# Evaluation\n",
    "forecast_mae = mean_absolute_error(y_forecast_test, y_forecast_pred)\n",
    "forecast_rmse = np.sqrt(mean_squared_error(y_forecast_test, y_forecast_pred))\n",
    "forecast_r2 = r2_score(y_forecast_test, y_forecast_pred)\n",
    "\n",
    "print(f\"Sales Forecast Model Performance:\")\n",
    "print(f\"MAE: {forecast_mae:.2f}\")\n",
    "print(f\"RMSE: {forecast_rmse:.2f}\")\n",
    "print(f\"R²: {forecast_r2:.3f}\")\n",
    "\n",
    "print(\"All models training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c970c5",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Comprehensive evaluation of all trained models with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48352dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Summary\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "performance_summary = pd.DataFrame({\n",
    "    'Model': ['Customer Lifetime Value', 'Customer Churn Prediction', 'Sales Forecasting'],\n",
    "    'Type': ['Regression', 'Classification', 'Regression'],\n",
    "    'Primary Metric': [f'R² = {clv_r2:.3f}', f'Accuracy = {churn_accuracy:.3f}', f'R² = {forecast_r2:.3f}'],\n",
    "    'Secondary Metric': [f'MAE = {clv_mae:.0f}', 'F1-Score from Report', f'MAE = {forecast_mae:.0f}']\n",
    "})\n",
    "\n",
    "print(performance_summary.to_string(index=False))\n",
    "print(\"\\nCustomer Segmentation: 5 distinct segments created\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# CLV Predictions vs Actual\n",
    "axes[0,0].scatter(y_clv_test, y_clv_pred, alpha=0.6)\n",
    "axes[0,0].plot([y_clv_test.min(), y_clv_test.max()], [y_clv_test.min(), y_clv_test.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual CLV')\n",
    "axes[0,0].set_ylabel('Predicted CLV')\n",
    "axes[0,0].set_title(f'CLV Prediction (R² = {clv_r2:.3f})')\n",
    "\n",
    "# Churn Feature Importance\n",
    "axes[0,1].barh(churn_importance.head(8)['feature'], churn_importance.head(8)['importance'])\n",
    "axes[0,1].set_title('Top Churn Prediction Features')\n",
    "axes[0,1].set_xlabel('Importance')\n",
    "\n",
    "# Sales Forecast\n",
    "axes[1,0].scatter(y_forecast_test, y_forecast_pred, alpha=0.6)\n",
    "axes[1,0].plot([y_forecast_test.min(), y_forecast_test.max()], [y_forecast_test.min(), y_forecast_test.max()], 'r--', lw=2)\n",
    "axes[1,0].set_xlabel('Actual Sales')\n",
    "axes[1,0].set_ylabel('Predicted Sales')\n",
    "axes[1,0].set_title(f'Sales Forecasting (R² = {forecast_r2:.3f})')\n",
    "\n",
    "# Customer Segments\n",
    "segment_counts = X_segment['segment_name'].value_counts()\n",
    "axes[1,1].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Customer Segments Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff228483",
   "metadata": {},
   "source": [
    "## 7. Save Models as PKL Files\n",
    "\n",
    "Use pickle to serialize and save all trained models and scalers for use in Streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models and preprocessors as PKL files\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "models_dir = 'models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "print(\"Saving all trained models as PKL files...\")\n",
    "\n",
    "# 1. CLV Model and Scaler\n",
    "with open('models/clv_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_clv, f)\n",
    "    \n",
    "with open('models/clv_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_clv, f)\n",
    "\n",
    "# 2. Customer Segmentation Model and Scaler\n",
    "with open('models/segmentation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "    \n",
    "with open('models/segmentation_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_segment, f)\n",
    "\n",
    "# 3. Churn Prediction Model and Scaler\n",
    "with open('models/churn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_churn, f)\n",
    "    \n",
    "with open('models/churn_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_churn, f)\n",
    "\n",
    "# 4. Sales Forecasting Model\n",
    "with open('models/sales_forecast_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_forecast, f)\n",
    "\n",
    "# 5. Save feature names and model metadata\n",
    "model_metadata = {\n",
    "    'clv_features': clv_features,\n",
    "    'churn_features': churn_features,\n",
    "    'forecast_features': forecast_features,\n",
    "    'segmentation_features': segmentation_features,\n",
    "    'segment_mapping': segment_mapping,\n",
    "    'model_performance': {\n",
    "        'clv_r2': clv_r2,\n",
    "        'clv_mae': clv_mae,\n",
    "        'churn_accuracy': churn_accuracy,\n",
    "        'forecast_r2': forecast_r2,\n",
    "        'forecast_mae': forecast_mae\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(model_metadata, f)\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "for file in os.listdir('models'):\n",
    "    print(f\"- models/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f47900",
   "metadata": {},
   "source": [
    "## 8. Create Streamlit Application\n",
    "\n",
    "Build a comprehensive Streamlit web application that loads the pkl files and provides interfaces for all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit Application Code\n",
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"AdventureWorks Analytics\",\n",
    "    page_icon=\"📊\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Load models and metadata\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    \"\"\"Load all trained models and preprocessors\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Load CLV model\n",
    "    with open('models/clv_model.pkl', 'rb') as f:\n",
    "        models['clv_model'] = pickle.load(f)\n",
    "    with open('models/clv_scaler.pkl', 'rb') as f:\n",
    "        models['clv_scaler'] = pickle.load(f)\n",
    "    \n",
    "    # Load segmentation model\n",
    "    with open('models/segmentation_model.pkl', 'rb') as f:\n",
    "        models['segmentation_model'] = pickle.load(f)\n",
    "    with open('models/segmentation_scaler.pkl', 'rb') as f:\n",
    "        models['segmentation_scaler'] = pickle.load(f)\n",
    "    \n",
    "    # Load churn model\n",
    "    with open('models/churn_model.pkl', 'rb') as f:\n",
    "        models['churn_model'] = pickle.load(f)\n",
    "    with open('models/churn_scaler.pkl', 'rb') as f:\n",
    "        models['churn_scaler'] = pickle.load(f)\n",
    "    \n",
    "    # Load sales forecast model\n",
    "    with open('models/sales_forecast_model.pkl', 'rb') as f:\n",
    "        models['sales_forecast_model'] = pickle.load(f)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open('models/model_metadata.pkl', 'rb') as f:\n",
    "        models['metadata'] = pickle.load(f)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Main application\n",
    "def main():\n",
    "    st.title(\"🚀 AdventureWorks Predictive Analytics Dashboard\")\n",
    "    st.markdown(\"Comprehensive ML-powered business insights and predictions\")\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models()\n",
    "    \n",
    "    # Sidebar navigation\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    page = st.sidebar.selectbox(\"Choose Analysis\", [\n",
    "        \"📊 Dashboard Overview\",\n",
    "        \"💰 Customer Lifetime Value\",\n",
    "        \"👥 Customer Segmentation\", \n",
    "        \"⚠️ Churn Prediction\",\n",
    "        \"📈 Sales Forecasting\"\n",
    "    ])\n",
    "    \n",
    "    # Dashboard Overview\n",
    "    if page == \"📊 Dashboard Overview\":\n",
    "        st.header(\"Model Performance Overview\")\n",
    "        \n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        perf = models['metadata']['model_performance']\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"CLV Model R²\", f\"{perf['clv_r2']:.3f}\")\n",
    "        with col2:\n",
    "            st.metric(\"Churn Accuracy\", f\"{perf['churn_accuracy']:.3f}\")\n",
    "        with col3:\n",
    "            st.metric(\"Sales Forecast R²\", f\"{perf['forecast_r2']:.3f}\")\n",
    "        with col4:\n",
    "            st.metric(\"Total Models\", \"4\")\n",
    "        \n",
    "        st.markdown(\"\"\"\n",
    "        ### Available Predictions:\n",
    "        - **Customer Lifetime Value**: Predict future customer worth\n",
    "        - **Customer Segmentation**: Identify customer groups\n",
    "        - **Churn Prediction**: Identify at-risk customers\n",
    "        - **Sales Forecasting**: Predict future sales trends\n",
    "        \"\"\")\n",
    "    \n",
    "    # Customer Lifetime Value\n",
    "    elif page == \"💰 Customer Lifetime Value\":\n",
    "        st.header(\"Customer Lifetime Value Prediction\")\n",
    "        \n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Customer Information\")\n",
    "            annual_income = st.number_input(\"Annual Income ($)\", min_value=0, max_value=200000, value=50000)\n",
    "            total_children = st.number_input(\"Number of Children\", min_value=0, max_value=10, value=1)\n",
    "            age = st.number_input(\"Age\", min_value=18, max_value=100, value=35)\n",
    "            gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "            marital_status = st.selectbox(\"Marital Status\", [\"Married\", \"Single\"])\n",
    "            education = st.selectbox(\"Education Level\", [\"High School\", \"Bachelors\", \"Graduate Degree\"])\n",
    "            total_orders = st.number_input(\"Historical Orders\", min_value=0, max_value=100, value=5)\n",
    "            total_quantity = st.number_input(\"Total Items Purchased\", min_value=0, max_value=1000, value=25)\n",
    "            avg_order_size = st.number_input(\"Average Order Size\", min_value=0.0, max_value=50.0, value=5.0)\n",
    "            lifespan_days = st.number_input(\"Customer Lifespan (days)\", min_value=0, max_value=3650, value=365)\n",
    "            \n",
    "            if st.button(\"Predict CLV\"):\n",
    "                # Prepare features\n",
    "                features = np.array([[\n",
    "                    annual_income, total_children, age, 1 if gender == \"Male\" else 0,\n",
    "                    1 if marital_status == \"Married\" else 0, 2 if education == \"Bachelors\" else (3 if education == \"Graduate Degree\" else 1),\n",
    "                    total_orders, total_quantity, avg_order_size, lifespan_days\n",
    "                ]])\n",
    "                \n",
    "                # Scale and predict\n",
    "                features_scaled = models['clv_scaler'].transform(features)\n",
    "                clv_prediction = models['clv_model'].predict(features_scaled)[0]\n",
    "                \n",
    "                with col2:\n",
    "                    st.subheader(\"CLV Prediction Results\")\n",
    "                    st.success(f\"Predicted Customer Lifetime Value: ${clv_prediction:,.2f}\")\n",
    "                    \n",
    "                    # CLV category\n",
    "                    if clv_prediction > 1000:\n",
    "                        category = \"High Value Customer\"\n",
    "                        color = \"green\"\n",
    "                    elif clv_prediction > 500:\n",
    "                        category = \"Medium Value Customer\"\n",
    "                        color = \"orange\"\n",
    "                    else:\n",
    "                        category = \"Low Value Customer\" \n",
    "                        color = \"red\"\n",
    "                    \n",
    "                    st.markdown(f\"Customer Category: :{color}[{category}]\")\n",
    "    \n",
    "    # Customer Segmentation\n",
    "    elif page == \"👥 Customer Segmentation\":\n",
    "        st.header(\"Customer Segmentation Analysis\")\n",
    "        \n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Customer Profile\")\n",
    "            annual_income = st.number_input(\"Annual Income\", min_value=0, max_value=200000, value=50000)\n",
    "            total_children = st.number_input(\"Children\", min_value=0, max_value=10, value=1)\n",
    "            age = st.number_input(\"Age\", min_value=18, max_value=100, value=35)\n",
    "            total_orders = st.number_input(\"Total Orders\", min_value=0, max_value=100, value=5)\n",
    "            total_quantity = st.number_input(\"Total Quantity\", min_value=0, max_value=1000, value=25)\n",
    "            avg_order_size = st.number_input(\"Avg Order Size\", min_value=0.0, max_value=50.0, value=5.0)\n",
    "            recency_days = st.number_input(\"Days Since Last Order\", min_value=0, max_value=1000, value=30)\n",
    "            \n",
    "            if st.button(\"Identify Segment\"):\n",
    "                # Prepare features\n",
    "                features = np.array([[annual_income, total_children, age, total_orders, total_quantity, avg_order_size, recency_days]])\n",
    "                \n",
    "                # Scale and predict\n",
    "                features_scaled = models['segmentation_scaler'].transform(features)\n",
    "                segment = models['segmentation_model'].predict(features_scaled)[0]\n",
    "                segment_name = models['metadata']['segment_mapping'][segment]\n",
    "                \n",
    "                with col2:\n",
    "                    st.subheader(\"Segmentation Results\")\n",
    "                    st.success(f\"Customer Segment: {segment_name}\")\n",
    "                    \n",
    "                    # Segment characteristics\n",
    "                    segment_info = {\n",
    "                        'Low Value': 'Infrequent buyers with low spend',\n",
    "                        'Occasional': 'Moderate buyers with average spend', \n",
    "                        'Regular': 'Consistent buyers with good spend',\n",
    "                        'High Value': 'Frequent buyers with high spend',\n",
    "                        'Premium': 'Top-tier customers with highest value'\n",
    "                    }\n",
    "                    \n",
    "                    st.info(segment_info.get(segment_name, \"Unknown segment\"))\n",
    "    \n",
    "    # Churn Prediction\n",
    "    elif page == \"⚠️ Churn Prediction\":\n",
    "        st.header(\"Customer Churn Prediction\")\n",
    "        \n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Customer Data\")\n",
    "            annual_income = st.number_input(\"Annual Income\", min_value=0, value=50000)\n",
    "            total_children = st.number_input(\"Children\", min_value=0, value=1)\n",
    "            age = st.number_input(\"Age\", min_value=18, value=35)\n",
    "            gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "            marital_status = st.selectbox(\"Marital Status\", [\"Married\", \"Single\"])\n",
    "            education = st.selectbox(\"Education\", [\"High School\", \"Bachelors\", \"Graduate Degree\"])\n",
    "            total_orders = st.number_input(\"Total Orders\", min_value=0, value=5)\n",
    "            avg_order_size = st.number_input(\"Avg Order Size\", min_value=0.0, value=5.0)\n",
    "            lifespan_days = st.number_input(\"Customer Lifespan (days)\", min_value=0, value=365)\n",
    "            recency_days = st.number_input(\"Days Since Last Order\", min_value=0, value=30)\n",
    "            \n",
    "            if st.button(\"Predict Churn Risk\"):\n",
    "                # Prepare features\n",
    "                features = np.array([[\n",
    "                    annual_income, total_children, age, 1 if gender == \"Male\" else 0,\n",
    "                    1 if marital_status == \"Married\" else 0, 2 if education == \"Bachelors\" else (3 if education == \"Graduate Degree\" else 1),\n",
    "                    total_orders, avg_order_size, lifespan_days, recency_days\n",
    "                ]])\n",
    "                \n",
    "                # Scale and predict\n",
    "                features_scaled = models['churn_scaler'].transform(features)\n",
    "                churn_prob = models['churn_model'].predict_proba(features_scaled)[0][1]\n",
    "                churn_prediction = models['churn_model'].predict(features_scaled)[0]\n",
    "                \n",
    "                with col2:\n",
    "                    st.subheader(\"Churn Prediction Results\")\n",
    "                    \n",
    "                    if churn_prediction == 1:\n",
    "                        st.error(f\"High Churn Risk: {churn_prob:.2%} probability\")\n",
    "                        st.markdown(\"**Recommended Actions:**\")\n",
    "                        st.markdown(\"- Send retention offers\")\n",
    "                        st.markdown(\"- Personalized engagement campaign\")\n",
    "                        st.markdown(\"- Customer service outreach\")\n",
    "                    else:\n",
    "                        st.success(f\"Low Churn Risk: {churn_prob:.2%} probability\")\n",
    "                        st.markdown(\"**Customer Status:** Likely to remain active\")\n",
    "                    \n",
    "                    # Probability gauge\n",
    "                    fig = go.Figure(go.Indicator(\n",
    "                        mode = \"gauge+number\",\n",
    "                        value = churn_prob * 100,\n",
    "                        domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "                        title = {'text': \"Churn Probability (%)\"},\n",
    "                        gauge = {\n",
    "                            'axis': {'range': [None, 100]},\n",
    "                            'bar': {'color': \"red\" if churn_prob > 0.5 else \"green\"},\n",
    "                            'steps': [\n",
    "                                {'range': [0, 50], 'color': \"lightgray\"},\n",
    "                                {'range': [50, 100], 'color': \"gray\"}],\n",
    "                            'threshold': {\n",
    "                                'line': {'color': \"red\", 'width': 4},\n",
    "                                'thickness': 0.75,\n",
    "                                'value': 90}}))\n",
    "                    st.plotly_chart(fig)\n",
    "    \n",
    "    # Sales Forecasting\n",
    "    elif page == \"📈 Sales Forecasting\":\n",
    "        st.header(\"Sales Forecasting\")\n",
    "        \n",
    "        st.subheader(\"Forecast Parameters\")\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            month = st.selectbox(\"Month\", range(1, 13), index=5)\n",
    "        with col2:\n",
    "            day = st.selectbox(\"Day\", range(1, 32), index=14)\n",
    "        with col3:\n",
    "            dayofweek = st.selectbox(\"Day of Week\", range(0, 7), index=2)\n",
    "        with col4:\n",
    "            quarter = st.selectbox(\"Quarter\", [1, 2, 3, 4], index=1)\n",
    "        \n",
    "        col5, col6 = st.columns(2)\n",
    "        with col5:\n",
    "            unique_customers = st.number_input(\"Expected Unique Customers\", min_value=1, value=100)\n",
    "            unique_products = st.number_input(\"Expected Unique Products\", min_value=1, value=50)\n",
    "        \n",
    "        with col6:\n",
    "            lag_1 = st.number_input(\"Previous Day Sales\", min_value=0, value=150)\n",
    "            lag_7 = st.number_input(\"Same Day Last Week Sales\", min_value=0, value=140)\n",
    "        \n",
    "        rolling_mean_7 = st.number_input(\"7-Day Average Sales\", min_value=0, value=145)\n",
    "        \n",
    "        if st.button(\"Generate Forecast\"):\n",
    "            # Prepare features\n",
    "            features = np.array([[month, day, dayofweek, quarter, unique_customers, unique_products, lag_1, lag_7, rolling_mean_7]])\n",
    "            \n",
    "            # Predict\n",
    "            forecast = models['sales_forecast_model'].predict(features)[0]\n",
    "            \n",
    "            st.success(f\"Predicted Sales Quantity: {forecast:.0f} units\")\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(range(1, 8)),\n",
    "                y=[lag_7, 135, 142, 138, 141, lag_1, forecast],\n",
    "                mode='lines+markers',\n",
    "                name='Sales Trend',\n",
    "                line=dict(color='blue', width=3),\n",
    "                marker=dict(size=8)\n",
    "            ))\n",
    "            fig.update_layout(\n",
    "                title='Sales Forecast Visualization',\n",
    "                xaxis_title='Time Period',\n",
    "                yaxis_title='Sales Quantity',\n",
    "                showlegend=True\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save Streamlit application\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"Streamlit application created successfully!\")\n",
    "print(\"File saved as: streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8ed86",
   "metadata": {},
   "source": [
    "## 9. Test the Streamlit App\n",
    "\n",
    "Instructions to run the Streamlit application locally and test its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35209b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for running the Streamlit app\n",
    "instructions = \"\"\"\n",
    "TO RUN THE STREAMLIT APPLICATION:\n",
    "\n",
    "1. Open terminal/command prompt\n",
    "2. Navigate to the project directory\n",
    "3. Run the following command:\n",
    "   \n",
    "   streamlit run streamlit_app.py\n",
    "\n",
    "4. The app will open in your browser at http://localhost:8501\n",
    "\n",
    "FEATURES OF THE APP:\n",
    "- Dashboard Overview with model performance metrics\n",
    "- Customer Lifetime Value prediction with interactive inputs\n",
    "- Customer Segmentation analysis\n",
    "- Churn Prediction with probability visualization  \n",
    "- Sales Forecasting with trend charts\n",
    "\n",
    "TESTING CHECKLIST:\n",
    "□ All models load successfully\n",
    "□ CLV predictions work with different input values\n",
    "□ Customer segmentation identifies correct segments\n",
    "□ Churn prediction shows probability gauge\n",
    "□ Sales forecasting generates reasonable predictions\n",
    "□ All visualizations render correctly\n",
    "\n",
    "The app uses the trained PKL files for real-time predictions!\n",
    "\"\"\"\n",
    "\n",
    "print(instructions)\n",
    "\n",
    "# Create a requirements.txt file for easy deployment\n",
    "requirements = \"\"\"\n",
    "streamlit\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "plotly\n",
    "pickle-mixin\n",
    "\"\"\"\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"\\nrequirements.txt file created for easy deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed6d14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully created a comprehensive machine learning pipeline for AdventureWorks:\n",
    "\n",
    "### Models Created & Saved:\n",
    "1. **Customer Lifetime Value (CLV)** - Random Forest Regression\n",
    "2. **Customer Segmentation** - KMeans Clustering (5 segments)\n",
    "3. **Customer Churn Prediction** - Random Forest Classification\n",
    "4. **Sales Forecasting** - Random Forest Regression\n",
    "\n",
    "### PKL Files Generated:\n",
    "- `clv_model.pkl` & `clv_scaler.pkl`\n",
    "- `segmentation_model.pkl` & `segmentation_scaler.pkl` \n",
    "- `churn_model.pkl` & `churn_scaler.pkl`\n",
    "- `sales_forecast_model.pkl`\n",
    "- `model_metadata.pkl`\n",
    "\n",
    "### Streamlit Application:\n",
    "- Interactive web interface for all predictions\n",
    "- Real-time model inference using PKL files\n",
    "- Professional dashboard with visualizations\n",
    "- Business-ready insights and recommendations\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run `streamlit run streamlit_app.py` to launch the application\n",
    "2. Test all prediction features\n",
    "3. Deploy to cloud platform if needed\n",
    "4. Integrate with business systems for automated insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
